---
title: "93% on Legal RAG Without Embeddings. No Vector DB. Just File Search."
date: "2026-02-22"
excerpt: "Everyone says you need embeddings, vector databases, and retrieval pipelines for RAG. We scored 93% correctness on 100 legal questions using grep and structured prompting. Here's exactly how."
template: "technical"
category: "AI Engineering"
---

93% correctness. 97% groundedness. 94% retrieval accuracy.

100 legal questions. Victorian criminal law — jury directions, evidence rules, homicide defences, drug trafficking, stalking.

No embeddings. No vector database. No retrieval pipeline. Just an AI agent searching a raw JSONL file with `grep` and `read`.

I'm sharing the exact prompts, the full results, and the code. Run it yourself.

![93% Correctness on Legal RAG Bench — head to head vs embedding pipelines](/images/legal-rag-hero.png)
*Legal RAG Bench: agentic file-search (grep + structured prompt) vs. 5 standard embedding+LLM pipelines. Same corpus, same questions.*

---

## The Setup

[Isaacus](https://isaacus.com/blog/legal-rag-bench) published Legal RAG Bench — a benchmark for testing RAG systems on legal questions. They evaluated 5 standard embedding+LLM pipelines and found that "information retrieval sets the ceiling on the performance of modern legal RAG systems."

Their best result? Karon 2 Embedder + Gemini 3.1 Pro at **83.6%** correctness.

We took the same corpus, the same 100 questions, and threw away the entire retrieval stack.

No embedding model. No chunking. No vector similarity. No top-k retrieval.

Instead: one AI agent per question, with access to `grep` and `read`, searching the raw corpus file directly. The LLM decides what to search for, what to read, and what to look at next.

**93% correctness.** Ten points above their best.

---

## Why Embeddings Are the Wrong Tool for Legal Documents

Legal text is adversarial to embedding-based retrieval. Here's why:

**Cross-references everywhere.** "As defined in Section 3.1(a)" appears constantly. Embeddings retrieve one chunk without the definition. File-search lets the agent navigate to it.

**Multiple rules in one passage.** A single passage might contain 5 numbered legal rules. Embeddings retrieve the passage. They can't tell the agent which of the 5 rules matters. Our agent enumerates all 5 and matches each to the question's facts.

**Negation kills similarity.** "This requirement does NOT apply to entities classified under Category B." Embedding similarity can't reliably distinguish "applies" from "does not apply." The agent reads the actual text.

**Specificity matters more than relevance.** A general legal principle and a specific exception might both be "relevant" to a question. Embeddings rank by similarity. Our agent ranks by specificity — the specific exception always wins.

---

## The Architecture

Two phases per question. One isolated agent session. No context leakage.

![Architecture comparison — Standard RAG Pipeline vs Agentic File-Search](/images/legal-rag-architecture.png)
*Left: Standard RAG (chunk → embed → top-k → generate). Right: Our approach (grep → read → ANALYZE → answer → verify). The ANALYZE phase is the difference.*

### Phase 1: Answer (Agent with tools)

The agent gets the question and access to the corpus via `grep` and `read`. It runs a 4-step structured prompt:

**GATHER** → Search with 2-3 different keyword strategies. Read full passages.

**ANALYZE** → For each retrieved passage:
- Extract every distinct rule/principle
- Quote the key sentence from each
- Match scenario facts to rules
- Rank matches by specificity (specific > general)

**ANSWER** → Synthesize using only retrieved evidence. Lead with the most specific rule. Every claim must trace to a verbatim quote.

**VERIFY** → Re-read each claim. Check for unsupported information. Run a specificity check: is the answer leading with the most specific rule, or did it default to a general principle?

### Phase 2: Judge (No tools)

Same session, tools removed. The judge sees the question, gold answer, candidate answer, and evidence. Scores correctness and groundedness as binary (pass/fail).

### That's it.

No embedding model. No vector database. No chunking strategy. No reranking. No hybrid search. No RAG pipeline.

`grep` + `read` + structured prompting.

---

## The Prompt Evolution: From 67% to 93%

This is the part that matters. The architecture is simple. The prompt engineering is where the accuracy lives.

![Before vs After — Standard RAG picks Rule #2, ANALYZE picks Rule #5](/images/legal-rag-before-after.gif)
*Same passage. Same question. Standard RAG grabs the first plausible rule. The ANALYZE phase enumerates all rules, matches to facts, and ranks by specificity.*

### v1 — Flat instruction (baseline): 67% correctness

```
You are a legal RAG assistant.
Search corpus.jsonl with available tools.
Do not use external knowledge.
Return JSON with answer and evidence.
```

Failed on a jury directions question. The passage had 5 rules. The agent grabbed the first plausible one (a general principle about jurors using their senses) instead of the specific one that matched the scenario (the condition of physical evidence may change over time).

**Failure mode:** No structured reasoning. Agent satisfices with the first plausible match.

### v2 — Gather → Answer → Verify: 67% correctness

Added multi-search strategy and self-verification. Groundedness improved to 100%. But the same question still failed — the agent found the right passage, saw 5 rules, and still picked the general one.

**Failure mode:** Agent retrieves correctly but can't distinguish general principles from specific rules.

### v3 — Gather → Analyze → Answer → Verify: 93% correctness

![The Analyze Phase — fact-to-evidence matching on the locksmith/juror question](/images/legal-rag-analyze-phase.gif)
*The ANALYZE phase in action: extract all 5 rules from the passage, match each to scenario facts, rank by specificity. Rule #5 wins because "damaged lock = condition may have changed" is more specific than "jurors use their senses."*

Added the ANALYZE phase with explicit fact-to-evidence matching:

1. Extract every scenario fact from the question (people, objects, actions, legal context)
2. For each passage, list ALL distinct rules — treat each numbered point separately
3. Match facts to rules — find the rule that MOST DIRECTLY addresses each fact
4. Rank by specificity — specific mechanism > general principle
5. Answer MUST lead with the most specific matched rule

The jury question now works: agent enumerates all 5 rules, matches "damaged lock" to "condition of object may have changed," ranks it above "jurors use their senses," and leads with the specific rule.

**One prompt change. 67% → 93%.** The retrieval didn't change. The reasoning did.

---

## Head-to-Head: File Search vs. Standard RAG

Same corpus. Same questions. Different architecture.

### Correctness (answer matches gold)

| System | Score |
|--------|-------|
| **Ours (agentic file-search)** | **93%** |
| Karon 2 + Gemini 3.1 Pro | 83.6% |
| Karon 2 + GPT-5.2 | ~80% |
| Text Emb. 3 + Gemini 3.1 Pro | ~78% |
| Gemini Emb. + GPT-5.2 | ~67% |

### Groundedness (answer supported by evidence)

| System | Score |
|--------|-------|
| **Ours** | **97%** |
| Karon 2 + Gemini 3.1 Pro | ~96% |
| Others | ~74-89% |

### Retrieval Accuracy (correct passage found)

| System | Score |
|--------|-------|
| **Ours** | **94%** |
| Karon 2 Embedder (top) | ~91% |
| Gemini Embedding 001 (top) | ~88% |

Zero retrieval errors. Every incorrect answer was a reasoning mistake, not a retrieval failure.

*Isaacus scores estimated from their published blog and relative figures. Their interactive explorer has per-question breakdowns. Exact numbers may vary slightly. Source: [isaacus.com/blog/legal-rag-bench](https://isaacus.com/blog/legal-rag-bench)*

---

## The 7 Remaining Errors

Every failure documented. No hiding behind aggregate numbers.

### Hallucinations (3)

| Q | Topic | What went wrong |
|---|---|---|
| Q31 | DNA evidence (twins) | Agent said DNA sufficient; gold says identical twins can't be distinguished by nuclear DNA |
| Q44 | Conspiracy (safety duty) | Agent said guilty of conspiracy; gold says safety duty protects defendant class |
| Q85 | Child taking vs harbouring | Agent said "harbouring"; gold says driving = "taking" |

### Reasoning errors (4)

| Q | Topic | What went wrong |
|---|---|---|
| Q25 | Evidence classification | Said "recognition evidence"; gold says "comparison evidence" |
| Q51 | Murder element | Focused on wrong element of the admission |
| Q53 | Lawful excuse | Answered about "human being" definition instead of duress |
| Q62 | Dangerous driving | Focused on subjective awareness instead of separate offence argument |

All 7 involve subtle legal distinctions where the agent drew the wrong conclusion from the right evidence. The system retrieved the correct passage every time. It just reasoned incorrectly on edge cases.

---

## Why This Matters Beyond Legal

The key finding isn't "file search beats embeddings on legal docs."

It's this: **information retrieval was never the ceiling. Reasoning was.**

Isaacus concluded that "information retrieval sets the ceiling on the performance of modern legal RAG systems." Our results suggest the opposite. With zero retrieval errors, the ceiling is reasoning quality — and structured prompting (the ANALYZE phase) raises that ceiling significantly.

This applies to any domain where documents have:
- Multiple rules/principles in a single passage
- Cross-references between sections
- Specificity hierarchies (general principle vs. specific exception)
- Negation and conditions

Financial regulations. Aviation lease agreements. Insurance policies. Compliance frameworks. Medical guidelines.

If you're building RAG for any of these, you might be optimizing the wrong layer.

---

## Run It Yourself

Everything is open. Clone the repo, run the eval.

```bash
git clone https://github.com/satish860/legal-rag-bench
cd legal-rag-bench
npm install
node agentic-rag-eval.mjs --limit 100 --quiet-tools
```

Smoke test (3 questions, ~3 minutes):

```bash
node agentic-rag-eval.mjs --limit 3 --quiet-tools
```

Output files:
- `agentic-eval-results-*.jsonl` — per-question scores
- `agentic-eval-summary-*.json` — aggregate metrics

Requires: Node.js, [Pi coding agent SDK](https://github.com/mariozechner/pi-coding-agent), and an LLM API key.

---

## What I'd Do Next

1. **Counter-argument prompting** for the 3 hallucinations — force the agent to consider "what if the opposite conclusion is true?" before finalizing
2. **Pre-built search index** for speed — 77 seconds/question is fine for eval, too slow for production
3. **Multi-judge ensemble** for edge cases where legal distinctions are genuinely ambiguous
4. **Test on other legal domains** — contract analysis, regulatory compliance, case law research

---

*We build AI document intelligence systems for regulated industries — aviation leasing, financial compliance, legal technology. The systems that process real documents in production, not demos.*

*If your RAG pipeline is stuck at 80% and you've exhausted your embedding options, maybe the problem isn't retrieval.*

*It's reasoning.*

---

**Built with [Pi](https://github.com/mariozechner/pi-coding-agent) coding agent. Evaluated on [Legal RAG Bench](https://isaacus.com/blog/legal-rag-bench) by Isaacus.**
