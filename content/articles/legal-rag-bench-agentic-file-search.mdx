---
title: "93% on Legal RAG Without Embeddings. No Vector DB. Just File Search."
date: "2026-02-22"
excerpt: "Everyone says you need embeddings, vector databases, and retrieval pipelines for RAG. We scored 93% correctness on 100 legal questions using grep and structured prompting. Here's exactly how."
template: "technical"
category: "AI Engineering"
keywords:
  - "legal RAG benchmark"
  - "RAG without embeddings"
  - "agentic RAG"
  - "legal document AI"
  - "retrieval augmented generation"
  - "Legal RAG Bench"
  - "structured prompting"
  - "file search vs vector database"
  - "AI legal research"
  - "RAG accuracy"
  - "embedding alternatives"
  - "agentic file search"
  - "document intelligence"
  - "legal AI benchmark"
  - "RAG pipeline optimization"
faq:
  - q: "Can you build RAG without embeddings or a vector database?"
    a: "Yes. We scored 93% correctness on the Legal RAG Bench benchmark using only grep and file-read tools with structured prompting — no embeddings, no vector database, no chunking. The LLM agent decides what to search for and navigates the raw corpus directly."
  - q: "How does agentic file search compare to traditional RAG pipelines?"
    a: "On Legal RAG Bench (100 legal questions), agentic file search with Claude Sonnet 4.6 scored 93% correctness — within 2 points of the best embedding pipeline (Kanon 2 + Gemini 3.1 Pro at 95%) and 16-20 points above every non-Kanon-2 pipeline. Retrieval accuracy was 94% vs. 86% for Kanon 2."
  - q: "What is the ANALYZE phase in structured prompting?"
    a: "The ANALYZE phase is a prompt engineering technique that forces the LLM to enumerate every rule in a retrieved passage, match each rule to the question's facts, and rank matches by specificity (specific exception beats general principle). Adding this single phase improved accuracy from 67% to 93%."
  - q: "Why do embeddings fail on legal documents?"
    a: "Legal text has cross-references between sections, multiple numbered rules in single passages, negation patterns that embedding similarity can't distinguish, and specificity hierarchies where a specific exception should override a general principle. Embeddings rank by similarity, not by legal specificity."
  - q: "What domains benefit from agentic file search over traditional RAG?"
    a: "Any domain with multi-rule passages, cross-references, specificity hierarchies, and negation — including financial regulations, aviation lease agreements, insurance policies, compliance frameworks, and medical guidelines."
---

> **Correction (Feb 23, 2026):** The original version of this post reported Isaacus's best result (Kanon 2 + Gemini 3.1 Pro) as 83.6% correctness. The actual number from their [raw results data](https://media.isaacus.com/posts/legal-rag-bench/legal-rag-bench-results.jsonl) is **95%**. Our 93% is 2 points *below* their best, not 10 above. I estimated from the blog text instead of pulling the actual JSONL results file. The numbers, tables, and analysis below have been corrected. The LLM used was Claude Sonnet 4.6. The core finding still holds: agentic file-search with zero embeddings matches or beats 5 of 6 standard pipelines — but Kanon 2's domain-adapted retrieval edges it out on correctness.

93% correctness. 97% groundedness. 94% retrieval accuracy.

100 legal questions. Victorian criminal law — jury directions, evidence rules, homicide defences, drug trafficking, stalking.

No embeddings. No vector database. No retrieval pipeline. Just an AI agent (Claude Sonnet 4.6) searching a raw JSONL file with `grep` and `read`.

I'm sharing the exact prompts, the full results, and the code. Run it yourself.

![93% Correctness on Legal RAG Bench — head to head vs embedding pipelines](/images/legal-rag-hero.png)
*Legal RAG Bench: agentic file-search (grep + structured prompt) vs. 5 standard embedding+LLM pipelines. Same corpus, same questions.*

---

## The Setup

[Isaacus](https://isaacus.com/blog/legal-rag-bench) published Legal RAG Bench — a benchmark for testing RAG systems on legal questions. They evaluated 5 standard embedding+LLM pipelines and found that "information retrieval sets the ceiling on the performance of modern legal RAG systems."

Their best result? Kanon 2 Embedder + Gemini 3.1 Pro at **95%** correctness.

We took the same corpus, the same 100 questions, and threw away the entire retrieval stack.

No embedding model. No chunking. No vector similarity. No top-k retrieval.

Instead: one AI agent (Claude Sonnet 4.6) per question, with access to `grep` and `read`, searching the raw corpus file directly. The LLM decides what to search for, what to read, and what to look at next.

**93% correctness.** Within 2 points of their best pipeline — and 16-20 points above every non-Kanon-2 pipeline. With zero embeddings.

---

## Why Embeddings Are the Wrong Tool for Legal Documents

Legal text is adversarial to embedding-based retrieval. Here's why:

**Cross-references everywhere.** "As defined in Section 3.1(a)" appears constantly. Embeddings retrieve one chunk without the definition. File-search lets the agent navigate to it.

**Multiple rules in one passage.** A single passage might contain 5 numbered legal rules. Embeddings retrieve the passage. They can't tell the agent which of the 5 rules matters. Our agent enumerates all 5 and matches each to the question's facts.

**Negation kills similarity.** "This requirement does NOT apply to entities classified under Category B." Embedding similarity can't reliably distinguish "applies" from "does not apply." The agent reads the actual text.

**Specificity matters more than relevance.** A general legal principle and a specific exception might both be "relevant" to a question. Embeddings rank by similarity. Our agent ranks by specificity — the specific exception always wins.

---

## The Architecture

Two phases per question. One isolated agent session. No context leakage.

![Architecture comparison — Standard RAG Pipeline vs Agentic File-Search](/images/legal-rag-architecture.png)
*Left: Standard RAG (chunk → embed → top-k → generate). Right: Our approach (grep → read → ANALYZE → answer → verify). The ANALYZE phase is the difference.*

### Phase 1: Answer (Agent with tools)

The agent gets the question and access to the corpus via `grep` and `read`. It runs a 4-step structured prompt:

**GATHER** → Search with 2-3 different keyword strategies. Read full passages.

**ANALYZE** → For each retrieved passage:
- Extract every distinct rule/principle
- Quote the key sentence from each
- Match scenario facts to rules
- Rank matches by specificity (specific > general)

**ANSWER** → Synthesize using only retrieved evidence. Lead with the most specific rule. Every claim must trace to a verbatim quote.

**VERIFY** → Re-read each claim. Check for unsupported information. Run a specificity check: is the answer leading with the most specific rule, or did it default to a general principle?

### Phase 2: Judge (No tools)

Same session, tools removed. The judge sees the question, gold answer, candidate answer, and evidence. Scores correctness and groundedness as binary (pass/fail).

### That's it.

No embedding model. No vector database. No chunking strategy. No reranking. No hybrid search. No RAG pipeline.

`grep` + `read` + structured prompting.

---

## The Prompt Evolution: From 67% to 93%

This is the part that matters. The architecture is simple. The prompt engineering is where the accuracy lives.

![Before vs After — Standard RAG picks Rule #2, ANALYZE picks Rule #5](/images/legal-rag-before-after.gif)
*Same passage. Same question. Standard RAG grabs the first plausible rule. The ANALYZE phase enumerates all rules, matches to facts, and ranks by specificity.*

### v1 — Flat instruction (baseline): 67% correctness

```
You are a legal RAG assistant.
Search corpus.jsonl with available tools.
Do not use external knowledge.
Return JSON with answer and evidence.
```

Failed on a jury directions question. The passage had 5 rules. The agent grabbed the first plausible one (a general principle about jurors using their senses) instead of the specific one that matched the scenario (the condition of physical evidence may change over time).

**Failure mode:** No structured reasoning. Agent satisfices with the first plausible match.

### v2 — Gather → Answer → Verify: 67% correctness

Added multi-search strategy and self-verification. Groundedness improved to 100%. But the same question still failed — the agent found the right passage, saw 5 rules, and still picked the general one.

**Failure mode:** Agent retrieves correctly but can't distinguish general principles from specific rules.

### v3 — Gather → Analyze → Answer → Verify: 93% correctness

![The Analyze Phase — fact-to-evidence matching on the locksmith/juror question](/images/legal-rag-analyze-phase.gif)
*The ANALYZE phase in action: extract all 5 rules from the passage, match each to scenario facts, rank by specificity. Rule #5 wins because "damaged lock = condition may have changed" is more specific than "jurors use their senses."*

Added the ANALYZE phase with explicit fact-to-evidence matching:

1. Extract every scenario fact from the question (people, objects, actions, legal context)
2. For each passage, list ALL distinct rules — treat each numbered point separately
3. Match facts to rules — find the rule that MOST DIRECTLY addresses each fact
4. Rank by specificity — specific mechanism > general principle
5. Answer MUST lead with the most specific matched rule

The jury question now works: agent enumerates all 5 rules, matches "damaged lock" to "condition of object may have changed," ranks it above "jurors use their senses," and leads with the specific rule.

**One prompt change. 67% → 93%.** The retrieval didn't change. The reasoning did.

---

## Head-to-Head: File Search vs. Standard RAG

Same corpus. Same questions. Different architecture.

### Correctness (answer matches gold)

| System | Score |
|--------|-------|
| Kanon 2 + Gemini 3.1 Pro | **95%** |
| **Ours (agentic file-search, Claude Sonnet 4.6)** | **93%** |
| Kanon 2 + GPT-5.2 | 93% |
| Text Emb. 3 Large + Gemini 3.1 Pro | 77% |
| Text Emb. 3 Large + GPT-5.2 | 76% |
| Gemini Emb. 001 + Gemini 3.1 Pro | 75% |
| Gemini Emb. 001 + GPT-5.2 | 73% |

### Groundedness (answer supported by evidence)

| System | Score |
|--------|-------|
| Kanon 2 + GPT-5.2 | **97%** |
| **Ours** | **97%** |
| Text Emb. 3 Large + Gemini 3.1 Pro | 96% |
| Kanon 2 + Gemini 3.1 Pro | 95% |
| Gemini Emb. 001 + Gemini 3.1 Pro | 92% |
| Text Emb. 3 Large + GPT-5.2 | 87% |
| Gemini Emb. 001 + GPT-5.2 | 82% |

### Retrieval Accuracy (correct passage found)

| System | Score |
|--------|-------|
| **Ours** | **94%** |
| Kanon 2 Embedder | 86% |
| Gemini Emb. 001 | 53% |
| Text Emb. 3 Large | 52% |

Our retrieval accuracy (94%) beats every embedding model — including Kanon 2 (86%). Every incorrect answer was a reasoning mistake, not a retrieval failure.

*Isaacus scores computed from their raw results file: [legal-rag-bench-results.jsonl](https://media.isaacus.com/posts/legal-rag-bench/legal-rag-bench-results.jsonl) (600 evaluations, 100 questions × 6 model combinations). Source: [isaacus.com/blog/legal-rag-bench](https://isaacus.com/blog/legal-rag-bench)*

---

## The 7 Remaining Errors

Every failure documented. No hiding behind aggregate numbers.

### Hallucinations (3)

| Q | Topic | What went wrong |
|---|---|---|
| Q31 | DNA evidence (twins) | Agent said DNA sufficient; gold says identical twins can't be distinguished by nuclear DNA |
| Q44 | Conspiracy (safety duty) | Agent said guilty of conspiracy; gold says safety duty protects defendant class |
| Q85 | Child taking vs harbouring | Agent said "harbouring"; gold says driving = "taking" |

### Reasoning errors (4)

| Q | Topic | What went wrong |
|---|---|---|
| Q25 | Evidence classification | Said "recognition evidence"; gold says "comparison evidence" |
| Q51 | Murder element | Focused on wrong element of the admission |
| Q53 | Lawful excuse | Answered about "human being" definition instead of duress |
| Q62 | Dangerous driving | Focused on subjective awareness instead of separate offence argument |

All 7 involve subtle legal distinctions where the agent drew the wrong conclusion from the right evidence. The system retrieved the correct passage every time. It just reasoned incorrectly on edge cases.

---

## Why This Matters Beyond Legal

The key finding isn't "file search beats embeddings on legal docs." Kanon 2 + Gemini 3.1 Pro at 95% correctness is the best pipeline tested.

It's this: **you can get within 2 points of the best embedding pipeline with zero embeddings — and structured reasoning is what closes the gap.**

Isaacus concluded that "information retrieval sets the ceiling on the performance of modern legal RAG systems." Our results partially confirm and partially challenge this. Kanon 2's domain-adapted retrieval + Gemini 3.1 Pro's reasoning achieved 95% — the highest score. But our approach, with zero embeddings, hit 93% by investing entirely in structured reasoning. Against non-domain-adapted embeddings (Text Embedding 3 Large, Gemini Embedding 001), the agentic approach wins decisively — 16-20 points higher. The takeaway: if you have access to a domain-adapted legal embedder like Kanon 2, use it. If you don't, structured reasoning can close most of the gap.

This applies to any domain where documents have:
- Multiple rules/principles in a single passage
- Cross-references between sections
- Specificity hierarchies (general principle vs. specific exception)
- Negation and conditions

Financial regulations. Aviation lease agreements. Insurance policies. Compliance frameworks. Medical guidelines.

If you're building RAG for any of these, you might be optimizing the wrong layer.

---

## Run It Yourself

Everything is open. Clone the repo, run the eval.

```bash
git clone https://github.com/satish860/legal-rag-bench
cd legal-rag-bench
npm install
node agentic-rag-eval.mjs --limit 100 --quiet-tools
```

Smoke test (3 questions, ~3 minutes):

```bash
node agentic-rag-eval.mjs --limit 3 --quiet-tools
```

Output files:
- `agentic-eval-results-*.jsonl` — per-question scores
- `agentic-eval-summary-*.json` — aggregate metrics

Requires: Node.js, [Pi coding agent SDK](https://github.com/mariozechner/pi-coding-agent), and an LLM API key.

---

## What I'd Do Next

1. **Counter-argument prompting** for the 3 hallucinations — force the agent to consider "what if the opposite conclusion is true?" before finalizing
2. **Pre-built search index** for speed — 77 seconds/question is fine for eval, too slow for production
3. **Multi-judge ensemble** for edge cases where legal distinctions are genuinely ambiguous
4. **Test on other legal domains** — contract analysis, regulatory compliance, case law research

---

## Frequently Asked Questions

**Can you build RAG without embeddings or a vector database?**

Yes. We scored 93% correctness on the [Legal RAG Bench](https://isaacus.com/blog/legal-rag-bench) benchmark using only `grep` and file-read tools with structured prompting — no embeddings, no vector database, no chunking. The LLM agent decides what to search for and navigates the raw corpus directly.

**How does agentic file search compare to traditional RAG pipelines?**

On Legal RAG Bench (100 legal questions), agentic file search with Claude Sonnet 4.6 scored 93% correctness — within 2 points of the best embedding pipeline (Kanon 2 + Gemini 3.1 Pro at 95%) and 16-20 points above every non-Kanon-2 pipeline. Retrieval accuracy was 94% vs. 86% for Kanon 2, and groundedness was 97%. All results are reproducible — [run the eval yourself](https://github.com/satish860/legal-rag-bench).

**What is the ANALYZE phase in structured prompting?**

The ANALYZE phase is a prompt engineering technique that forces the LLM to enumerate every rule in a retrieved passage, match each rule to the question's facts, and rank matches by specificity (specific exception beats general principle). Adding this single phase improved accuracy from 67% to 93% — a 26 percentage point gain from one prompt change.

**Why do embeddings fail on legal documents?**

Legal text has cross-references between sections, multiple numbered rules in single passages, negation patterns that embedding similarity can't distinguish, and specificity hierarchies where a specific exception should override a general principle. Embeddings rank by similarity, not by legal specificity.

**What domains benefit from agentic file search over traditional RAG?**

Any domain with multi-rule passages, cross-references, specificity hierarchies, and negation — including financial regulations, aviation lease agreements, insurance policies, compliance frameworks, and medical guidelines. The key finding: information retrieval was never the ceiling — reasoning was.

---

*We build AI document intelligence systems for regulated industries — aviation leasing, financial compliance, legal technology. The systems that process real documents in production, not demos.*

*If your RAG pipeline is stuck at 80% and you've exhausted your embedding options, maybe the problem isn't retrieval.*

*It's reasoning.*

---

**Built with [Pi](https://github.com/mariozechner/pi-coding-agent) coding agent. Evaluated on [Legal RAG Bench](https://isaacus.com/blog/legal-rag-bench) by Isaacus.**
